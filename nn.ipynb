{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, Average, GRU, LSTMCell, RNN, Embedding, TimeDistributed, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils import tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from keras how-to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 128  # Latent dimensionality of the encoding space.\n",
    "num_samples = 50000  # Number of samples to train on.\n",
    "\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'data/old_eng_close.csv'\n",
    "# data_path = 'data/old_rus_all.csv'\n",
    "# data_path = 'data/old_eng_close.csv'\n",
    "# data_path = 'data/old_eng_close.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set(\"\\n\")\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split(',')[:2]\n",
    "    if (len(input_text) > 15) or (len(target_text) > 15):\n",
    "        continue\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 12596\n",
      "Number of unique input tokens: 42\n",
      "Number of unique output tokens: 28\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 17\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index['\\n']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index['\\n']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index['\\n']] = 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "def build_model(latent_dim, optimizer, encoder_dropout, decoder_dropout):\n",
    "    earlyStopping = EarlyStopping(monitor='val_acc', patience=10, verbose=0)\n",
    "#     mcp_save = ModelCheckpoint(file_name + '_e{epoch:02d}_v{val_loss:.2f}.hdf5', save_best_only=True, monitor='val_acc')\n",
    "    \n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(latent_dim, return_state=True, dropout=encoder_dropout)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=decoder_dropout)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 RMSprop 0.1 0.1\n",
      "best val_acc: 0.8283938392198104\n",
      "on epoch 99\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 RMSprop 0.1 0.3\n",
      "best val_acc: 0.8187775854677373\n",
      "on epoch 62\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 RMSprop 0.1 0.5\n",
      "best val_acc: 0.8167282262585978\n",
      "on epoch 74\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 RMSprop 0.3 0.1\n",
      "best val_acc: 0.8237996564159714\n",
      "on epoch 98\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 RMSprop 0.3 0.3\n",
      "best val_acc: 0.8211422412902984\n",
      "on epoch 98\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 RMSprop 0.3 0.5\n",
      "best val_acc: 0.816435457371278\n",
      "on epoch 90\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 RMSprop 0.5 0.1\n",
      "best val_acc: 0.813034858035457\n",
      "on epoch 73\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 RMSprop 0.5 0.3\n",
      "best val_acc: 0.8080803577604192\n",
      "on epoch 64\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 RMSprop 0.5 0.5\n",
      "best val_acc: 0.8078551466760737\n",
      "on epoch 97\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 Adam 0.1 0.1\n",
      "best val_acc: 0.8285289599373367\n",
      "on epoch 95\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 Adam 0.1 0.3\n",
      "best val_acc: 0.8286415688111625\n",
      "on epoch 99\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 Adam 0.1 0.5\n",
      "best val_acc: 0.8217728130667717\n",
      "on epoch 99\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 Adam 0.3 0.1\n",
      "best val_acc: 0.8300828786757967\n",
      "on epoch 99\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 Adam 0.3 0.3\n",
      "best val_acc: 0.8253310483411114\n",
      "on epoch 96\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 Adam 0.3 0.5\n",
      "best val_acc: 0.8201963769166429\n",
      "on epoch 91\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 Adam 0.5 0.1\n",
      "best val_acc: 0.8172687175262942\n",
      "on epoch 83\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 Adam 0.5 0.3\n",
      "best val_acc: 0.8198585680944084\n",
      "on epoch 95\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "64 Adam 0.5 0.5\n",
      "best val_acc: 0.8155571596545793\n",
      "on epoch 98\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 RMSprop 0.1 0.1\n",
      "best val_acc: 0.8308936142081689\n",
      "on epoch 50\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 RMSprop 0.1 0.3\n",
      "best val_acc: 0.8284613964751157\n",
      "on epoch 52\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 RMSprop 0.1 0.5\n",
      "best val_acc: 0.822538512954303\n",
      "on epoch 55\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 RMSprop 0.3 0.1\n",
      "best val_acc: 0.8332807852517226\n",
      "on epoch 56\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 RMSprop 0.3 0.3\n",
      "best val_acc: 0.8332132233412306\n",
      "on epoch 69\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 RMSprop 0.3 0.5\n",
      "best val_acc: 0.8260291885088268\n",
      "on epoch 53\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 RMSprop 0.5 0.1\n",
      "best val_acc: 0.8377173296525321\n",
      "on epoch 97\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 RMSprop 0.5 0.3\n",
      "best val_acc: 0.8341365646439709\n",
      "on epoch 71\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 RMSprop 0.5 0.5\n",
      "best val_acc: 0.8337537152478742\n",
      "on epoch 95\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 Adam 0.1 0.1\n",
      "best val_acc: 0.8344293317057272\n",
      "on epoch 48\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 Adam 0.1 0.3\n",
      "best val_acc: 0.8338888403467526\n",
      "on epoch 58\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 Adam 0.1 0.5\n",
      "best val_acc: 0.827988468578362\n",
      "on epoch 56\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 Adam 0.3 0.1\n",
      "best val_acc: 0.839406355416793\n",
      "on epoch 64\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 Adam 0.3 0.3\n",
      "best val_acc: 0.8361408888472167\n",
      "on epoch 76\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 Adam 0.3 0.5\n",
      "best val_acc: 0.8326276928688446\n",
      "on epoch 82\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 Adam 0.5 0.1\n",
      "best val_acc: 0.8424015842024437\n",
      "on epoch 98\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 Adam 0.5 0.3\n",
      "best val_acc: 0.8391811529125955\n",
      "on epoch 88\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "128 Adam 0.5 0.5\n",
      "best val_acc: 0.8345644520581408\n",
      "on epoch 93\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 RMSprop 0.1 0.1\n",
      "best val_acc: 0.8296549814035845\n",
      "on epoch 40\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 RMSprop 0.1 0.3\n",
      "best val_acc: 0.8243401480487806\n",
      "on epoch 28\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 RMSprop 0.1 0.5\n",
      "best val_acc: 0.8208719883541966\n",
      "on epoch 38\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 RMSprop 0.3 0.1\n",
      "best val_acc: 0.8339789219508675\n",
      "on epoch 36\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 RMSprop 0.3 0.3\n",
      "best val_acc: 0.8304206822951756\n",
      "on epoch 40\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 RMSprop 0.3 0.5\n",
      "best val_acc: 0.8270200855151435\n",
      "on epoch 45\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 RMSprop 0.5 0.1\n",
      "best val_acc: 0.8397441696244397\n",
      "on epoch 47\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 RMSprop 0.5 0.3\n",
      "best val_acc: 0.8347220993151526\n",
      "on epoch 48\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 RMSprop 0.5 0.5\n",
      "best val_acc: 0.8315917471649088\n",
      "on epoch 50\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 Adam 0.1 0.1\n",
      "best val_acc: 0.8340464811230144\n",
      "on epoch 30\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 Adam 0.1 0.3\n",
      "best val_acc: 0.8278758679195714\n",
      "on epoch 27\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 Adam 0.1 0.5\n",
      "best val_acc: 0.8267498392423481\n",
      "on epoch 23\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 Adam 0.3 0.1\n",
      "best val_acc: 0.8367939805911474\n",
      "on epoch 33\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 Adam 0.3 0.3\n",
      "best val_acc: 0.836501216724127\n",
      "on epoch 39\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 Adam 0.3 0.5\n",
      "best val_acc: 0.8319070383016403\n",
      "on epoch 37\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 Adam 0.5 0.1\n",
      "best val_acc: 0.8415683247776528\n",
      "on epoch 54\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 Adam 0.5 0.3\n",
      "best val_acc: 0.8390685566351571\n",
      "on epoch 60\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout\n",
      "256 Adam 0.5 0.5\n",
      "best val_acc: 0.8346545373133825\n",
      "on epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_24 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_23/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_23/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_26 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_25/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_25/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_28 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_27/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_27/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_30 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_29/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_29/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_32 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_31/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_31/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_34 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_33/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_33/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_36 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_35/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_35/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_38 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_37/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_37/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_40 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_39/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_39/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_42 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_41/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_41/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_44 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_43/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_43/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_46 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_45/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_45/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_48 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_47/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_47/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_50 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_49/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_49/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_52 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_51/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_51/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_54 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_53/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_53/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_56 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_55/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_55/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_58 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_57/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_57/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_60 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_59/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_59/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_62 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_61/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_61/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_64 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_63/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_63/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_66 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_65/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_65/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_68 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_67/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_67/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_70 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_69/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_69/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_72 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_71/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_71/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_74 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_73/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_73/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_76 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_75/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_75/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_78 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_77/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_77/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_80 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_79/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_79/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_82 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_81/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_81/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_84 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_83/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_83/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_86 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_85/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_85/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_88 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_87/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_87/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_90 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_89/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_89/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_92 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_91/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_91/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_94 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_93/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_93/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_96 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_95/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_95/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_98 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_97/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_97/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_100 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_99/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_99/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_102 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_101/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_101/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_104 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_103/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_103/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_106 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_105/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_105/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_108 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_107/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_107/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_110 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_109/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_109/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_112 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_111/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_111/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_114 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_113/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_113/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_116 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_115/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_115/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_118 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_117/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_117/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_120 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_119/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_119/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_122 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_121/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_121/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_124 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_123/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_123/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_126 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_125/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_125/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_128 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_127/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_127/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_130 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_129/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_129/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "hists = list()\n",
    "for latent_dim in [64, 128, 256]:\n",
    "    for optimizer in ['RMSprop', 'Adam']:\n",
    "        for encoder_dropout in [0.1, 0.3, 0.5]:\n",
    "            for decoder_dropout in [0.1, 0.3, 0.5]:\n",
    "                print()\n",
    "                print(\"latent_dim, optimizer, encoder_dropout, decoder_dropout\")\n",
    "                print(latent_dim, optimizer, encoder_dropout, decoder_dropout)\n",
    "                model = build_model(latent_dim, optimizer, encoder_dropout, decoder_dropout)\n",
    "                hist = model.fit(\n",
    "                    [encoder_input_data, decoder_input_data],\n",
    "                    decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[earlyStopping],\n",
    "                    verbose=0\n",
    "                )\n",
    "                m = max(hist.history[\"val_acc\"])\n",
    "                print(\"best val_acc:\", m)\n",
    "                print(\"on epoch\", hist.history[\"val_acc\"].index(m))\n",
    "                hists.append(hist)\n",
    "                \n",
    "with open(\"hists.pickle\", \"wb\") as f:\n",
    "    pickle.dump(hists, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "\n",
    "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def decode_target(ground_truth_vec):\n",
    "    indices_vec = np.argmax(ground_truth_vec[0, :, :], axis=1)\n",
    "    return \"\".join(reverse_target_char_index[c] for c in indices_vec).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input word:\tабидныи\n",
      "Decoded word:\tабидый\n",
      "Ground truth:\tобидный\n",
      "-\n",
      "Input word:\tаблань\n",
      "Decoded word:\tалоба\n",
      "Ground truth:\tяблоня\n",
      "-\n",
      "Input word:\tабланьныи\n",
      "Decoded word:\tабланный\n",
      "Ground truth:\tяблоневый\n",
      "-\n",
      "Input word:\tаблъко\n",
      "Decoded word:\tалобка\n",
      "Ground truth:\tяблоко\n",
      "-\n",
      "Input word:\tабрѣдь\n",
      "Decoded word:\tарода\n",
      "Ground truth:\tакрида\n",
      "-\n",
      "Input word:\tавгустьныи\n",
      "Decoded word:\tавгустный\n",
      "Ground truth:\tавгустный\n",
      "-\n",
      "Input word:\tавъгаръ\n",
      "Decoded word:\tавгра\n",
      "Ground truth:\tавгар\n",
      "-\n",
      "Input word:\tавъгустъ\n",
      "Decoded word:\tавгуст\n",
      "Ground truth:\tавгуст\n",
      "-\n",
      "Input word:\tавьныи\n",
      "Decoded word:\tавний\n",
      "Ground truth:\tявный\n",
      "-\n",
      "Input word:\tагода\n",
      "Decoded word:\tагода\n",
      "Ground truth:\tягода\n",
      "-\n",
      "Input word:\tадамантъ\n",
      "Decoded word:\tамман\n",
      "Ground truth:\tалмаз\n",
      "-\n",
      "Input word:\tадамасъ\n",
      "Decoded word:\tаммар\n",
      "Ground truth:\tалмаз\n",
      "-\n",
      "Input word:\tадовнии\n",
      "Decoded word:\tадовный\n",
      "Ground truth:\tадовый\n",
      "-\n",
      "Input word:\tадрила\n",
      "Decoded word:\tадилина\n",
      "Ground truth:\tветрила\n",
      "-\n",
      "Input word:\tадъвъ\n",
      "Decoded word:\tадво\n",
      "Ground truth:\tадовый\n",
      "-\n",
      "Input word:\tадьскыи\n",
      "Decoded word:\tадский\n",
      "Ground truth:\tадский\n",
      "-\n",
      "Input word:\tазамыслиѥ\n",
      "Decoded word:\tизмысль\n",
      "Ground truth:\tзамысел\n",
      "-\n",
      "Input word:\tазъбукъвьныи\n",
      "Decoded word:\tозверный\n",
      "Ground truth:\tазбучный\n",
      "-\n",
      "Input word:\tаирода\n",
      "Decoded word:\tарид\n",
      "Ground truth:\tаирода\n",
      "-\n",
      "Input word:\tакиань\n",
      "Decoded word:\tакина\n",
      "Ground truth:\tокеан\n",
      "-\n",
      "Input word:\tакорысь\n",
      "Decoded word:\tакорысь\n",
      "Ground truth:\tакорысь\n",
      "-\n",
      "Input word:\tаксамитныи\n",
      "Decoded word:\tаскамнятый\n",
      "Ground truth:\tаксамитный\n",
      "-\n",
      "Input word:\tалавастрьныи\n",
      "Decoded word:\tалавастровый\n",
      "Ground truth:\tалавастровый\n",
      "-\n",
      "Input word:\tалаксатисѧ\n",
      "Decoded word:\tокраситься\n",
      "Ground truth:\tоблачаться\n",
      "-\n",
      "Input word:\tалачюга\n",
      "Decoded word:\tлачуга\n",
      "Ground truth:\tлачуга\n",
      "-\n",
      "Input word:\tалмазъ\n",
      "Decoded word:\tалмаз\n",
      "Ground truth:\tалмаз\n",
      "-\n",
      "Input word:\tалнь\n",
      "Decoded word:\tальна\n",
      "Ground truth:\tлань\n",
      "-\n",
      "Input word:\tалои\n",
      "Decoded word:\tаловый\n",
      "Ground truth:\tалоэ\n",
      "-\n",
      "Input word:\tалоинъ\n",
      "Decoded word:\tалонь\n",
      "Ground truth:\tалойный\n",
      "-\n",
      "Input word:\tалтабасныи\n",
      "Decoded word:\tалтасть\n",
      "Ground truth:\tалтабасный\n",
      "-\n",
      "Input word:\tалфа\n",
      "Decoded word:\tалфа\n",
      "Ground truth:\tальфа\n",
      "-\n",
      "Input word:\tама\n",
      "Decoded word:\tама\n",
      "Ground truth:\tяма\n",
      "-\n",
      "Input word:\tамагиль\n",
      "Decoded word:\tамгиль\n",
      "Ground truth:\tамагиль\n",
      "-\n",
      "Input word:\tаметусонъ\n",
      "Decoded word:\tаметстус\n",
      "Ground truth:\tаметист\n",
      "-\n",
      "Input word:\tамини\n",
      "Decoded word:\tаминь\n",
      "Ground truth:\tаминь\n",
      "-\n",
      "Input word:\tамъфигоуи\n",
      "Decoded word:\tатмиргут\n",
      "Ground truth:\tамфигуи\n",
      "-\n",
      "Input word:\tанагностьскыи\n",
      "Decoded word:\tангна\n",
      "Ground truth:\tанагностический\n",
      "-\n",
      "Input word:\tаналогии\n",
      "Decoded word:\tанголосия\n",
      "Ground truth:\tаналогичный\n",
      "-\n",
      "Input word:\tанбаръ\n",
      "Decoded word:\tанбар\n",
      "Ground truth:\tамбар\n",
      "-\n",
      "Input word:\tангеловъ\n",
      "Decoded word:\tангелосово\n",
      "Ground truth:\tангельский\n",
      "-\n",
      "Input word:\tангелолѣпьныи\n",
      "Decoded word:\tангелоподибный\n",
      "Ground truth:\tангелоподобный\n",
      "-\n",
      "Input word:\tангелъ\n",
      "Decoded word:\tангел\n",
      "Ground truth:\tангел\n",
      "-\n",
      "Input word:\tангельнѣи\n",
      "Decoded word:\tангельский\n",
      "Ground truth:\tангельский\n",
      "-\n",
      "Input word:\tангельскыи\n",
      "Decoded word:\tангельский\n",
      "Ground truth:\tангельский\n",
      "-\n",
      "Input word:\tантипатъ\n",
      "Decoded word:\tантипат\n",
      "Ground truth:\tантипат\n",
      "-\n",
      "Input word:\tантихрищь\n",
      "Decoded word:\tантираг\n",
      "Ground truth:\tантихристный\n",
      "-\n",
      "Input word:\tаньхимандритъ\n",
      "Decoded word:\tанинограднит\n",
      "Ground truth:\tархимандрит\n",
      "-\n",
      "Input word:\tаприлии\n",
      "Decoded word:\tизпривый\n",
      "Ground truth:\tапрель\n",
      "-\n",
      "Input word:\tаприль\n",
      "Decoded word:\tаприль\n",
      "Ground truth:\tапрель\n",
      "-\n",
      "Input word:\tарижьныи\n",
      "Decoded word:\tарижный\n",
      "Ground truth:\tарижный\n",
      "-\n",
      "Input word:\tармячныи\n",
      "Decoded word:\tармачный\n",
      "Ground truth:\tармячный\n",
      "-\n",
      "Input word:\tархангеловъ\n",
      "Decoded word:\tархван\n",
      "Ground truth:\tархангелов\n",
      "-\n",
      "Input word:\tаспидовъ\n",
      "Decoded word:\tаспидный\n",
      "Ground truth:\tаспидный\n",
      "-\n",
      "Input word:\tаспидьныи\n",
      "Decoded word:\tаспидный\n",
      "Ground truth:\tаспидный\n",
      "-\n",
      "Input word:\tастрономия\n",
      "Decoded word:\tастрономия\n",
      "Ground truth:\tастрономия\n",
      "-\n",
      "Input word:\tасьныи\n",
      "Decoded word:\tасный\n",
      "Ground truth:\tясный\n",
      "-\n",
      "Input word:\tаугустъ\n",
      "Decoded word:\tавгуст\n",
      "Ground truth:\tавгуст\n",
      "-\n",
      "Input word:\tахатъ\n",
      "Decoded word:\tафта\n",
      "Ground truth:\tагат\n",
      "-\n",
      "Input word:\tбабии\n",
      "Decoded word:\tбабий\n",
      "Ground truth:\tбабий\n",
      "-\n",
      "Input word:\tбабинъ\n",
      "Decoded word:\tбабин\n",
      "Ground truth:\tбабий\n",
      "-\n",
      "Input word:\tбагроватисѧ\n",
      "Decoded word:\tбаговаться\n",
      "Ground truth:\tбагриться\n",
      "-\n",
      "Input word:\tбагряница\n",
      "Decoded word:\tбаграница\n",
      "Ground truth:\tбагряница\n",
      "-\n",
      "Input word:\tбаинѧ\n",
      "Decoded word:\tбаня\n",
      "Ground truth:\tбаня\n",
      "-\n",
      "Input word:\tбальныи\n",
      "Decoded word:\tбальный\n",
      "Ground truth:\tбанный\n",
      "-\n",
      "Input word:\tбальство\n",
      "Decoded word:\tбластество\n",
      "Ground truth:\tлекарство\n",
      "-\n",
      "Input word:\tбаньныи\n",
      "Decoded word:\tбанный\n",
      "Ground truth:\tбанный\n",
      "-\n",
      "Input word:\tбаньскыи\n",
      "Decoded word:\tбанный\n",
      "Ground truth:\tбанный\n",
      "-\n",
      "Input word:\tбаньчия\n",
      "Decoded word:\tбанния\n",
      "Ground truth:\tбанная\n",
      "-\n",
      "Input word:\tбаня\n",
      "Decoded word:\tбаня\n",
      "Ground truth:\tбаня\n",
      "-\n",
      "Input word:\tбанѧ\n",
      "Decoded word:\tбаня\n",
      "Ground truth:\tбаня\n",
      "-\n",
      "Input word:\tбархатити\n",
      "Decoded word:\tбархатить\n",
      "Ground truth:\tбархатить\n",
      "-\n",
      "Input word:\tбархатъ\n",
      "Decoded word:\tбархат\n",
      "Ground truth:\tбархат\n",
      "-\n",
      "Input word:\tбасмакъ\n",
      "Decoded word:\tбасмак\n",
      "Ground truth:\tбаскак\n",
      "-\n",
      "Input word:\tбасманникъ\n",
      "Decoded word:\tбастанник\n",
      "Ground truth:\tбасманник\n",
      "-\n",
      "Input word:\tбашмакъ\n",
      "Decoded word:\tбашмак\n",
      "Ground truth:\tбашмак\n",
      "-\n",
      "Input word:\tбашьнѧ\n",
      "Decoded word:\tбашня\n",
      "Ground truth:\tбашня\n",
      "-\n",
      "Input word:\tбебръ\n",
      "Decoded word:\tбебр\n",
      "Ground truth:\tбобр\n",
      "-\n",
      "Input word:\tбезаконениѥ\n",
      "Decoded word:\tбеззаконие\n",
      "Ground truth:\tбеззаконие\n",
      "-\n",
      "Input word:\tбезакониѥ\n",
      "Decoded word:\tбеззаконие\n",
      "Ground truth:\tбеззаконие\n",
      "-\n",
      "Input word:\tбезаконьникъ\n",
      "Decoded word:\tбеззаконник\n",
      "Ground truth:\tбеззаконник\n",
      "-\n",
      "Input word:\tбезаконьница\n",
      "Decoded word:\tбеззаконница\n",
      "Ground truth:\tбеззаконница\n",
      "-\n",
      "Input word:\tбезаконьно\n",
      "Decoded word:\tбеззаконно\n",
      "Ground truth:\tнезаконно\n",
      "-\n",
      "Input word:\tбезаконьныи\n",
      "Decoded word:\tбеззаконный\n",
      "Ground truth:\tнезаконный\n",
      "-\n",
      "Input word:\tбезаконьнѣ\n",
      "Decoded word:\tбеззаконно\n",
      "Ground truth:\tнезаконно\n",
      "-\n",
      "Input word:\tбезаконьство\n",
      "Decoded word:\tбезнаконно\n",
      "Ground truth:\tбеззаконие\n",
      "-\n",
      "Input word:\tбезблагодатьныи\n",
      "Decoded word:\tбезбагодатный\n",
      "Ground truth:\tнеблагодарный\n",
      "-\n",
      "Input word:\tбезблагодьникъ\n",
      "Decoded word:\tбезблагодерный\n",
      "Ground truth:\tнеблагодарный\n",
      "-\n",
      "Input word:\tбезблазньныи\n",
      "Decoded word:\tбезбоязненный\n",
      "Ground truth:\tнесоблазняемый\n",
      "-\n",
      "Input word:\tбезблудьно\n",
      "Decoded word:\tбезблодно\n",
      "Ground truth:\tбезошибочно\n",
      "-\n",
      "Input word:\tбезбогъ\n",
      "Decoded word:\tбезбог\n",
      "Ground truth:\tбезбожный\n",
      "-\n",
      "Input word:\tбезбожьникъ\n",
      "Decoded word:\tбезбожный\n",
      "Ground truth:\tбезбожник\n",
      "-\n",
      "Input word:\tбезбожьныи\n",
      "Decoded word:\tбезбожный\n",
      "Ground truth:\tбезбожный\n",
      "-\n",
      "Input word:\tбезбожьскыи\n",
      "Decoded word:\tбезбожный\n",
      "Ground truth:\tбезбожный\n",
      "-\n",
      "Input word:\tбезбожьствиѥ\n",
      "Decoded word:\tбезбожность\n",
      "Ground truth:\tбезбожность\n",
      "-\n",
      "Input word:\tбезболѣзньныи\n",
      "Decoded word:\tбезболезненный\n",
      "Ground truth:\tбезболезненный\n",
      "-\n",
      "Input word:\tбезбоязньныи\n",
      "Decoded word:\tбезбоязненный\n",
      "Ground truth:\tбезбоязненный\n",
      "-\n",
      "Input word:\tбезбоязньство\n",
      "Decoded word:\tбезбожность\n",
      "Ground truth:\tбезбоязность\n",
      "-\n",
      "Input word:\tбезбрадьнъ\n",
      "Decoded word:\tбезбрадный\n",
      "Ground truth:\tбезбородый\n",
      "-\n",
      "Input word:\tбезбрачьныи\n",
      "Decoded word:\tбезбрачный\n",
      "Ground truth:\tбезбрачный\n",
      "-\n",
      "Input word:\tбезбѣдныи\n",
      "Decoded word:\tбезбожный\n",
      "Ground truth:\tбезопасный\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    ground_truth = decode_target(decoder_target_data[seq_index: seq_index + 1])\n",
    "    decoded_sentence = decode_sequence(input_seq).strip()\n",
    "    print('-')\n",
    "    print('Input word:', input_texts[seq_index], sep=\"\\t\")\n",
    "    print('Decoded word:', decoded_sentence, sep=\"\\t\")\n",
    "    print(\"Ground truth:\", ground_truth, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "def build_model(latent_dim, optimizer, encoder_dropout, decoder_dropout):\n",
    "    earlyStopping = EarlyStopping(monitor='val_acc', patience=10, verbose=0)\n",
    "#     mcp_save = ModelCheckpoint(file_name + '_e{epoch:02d}_v{val_loss:.2f}.hdf5', save_best_only=True, monitor='val_acc')\n",
    "    \n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, recurrent_dropout=encoder_dropout))\n",
    "    \n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True, recurrent_dropout=decoder_dropout)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout, batch_size\n",
      "32 Adam 0.3 0.3 64\n",
      "best val_acc: 0.84112542262182\n",
      "on epoch 97\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout, batch_size\n",
      "32 Adam 0.3 0.3 128\n",
      "best val_acc: 0.8375521736098772\n",
      "on epoch 97\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout, batch_size\n",
      "32 Adam 0.3 0.5 64\n",
      "best val_acc: 0.839173642849058\n",
      "on epoch 96\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout, batch_size\n",
      "32 Adam 0.3 0.5 128\n",
      "best val_acc: 0.8305858287536227\n",
      "on epoch 99\n",
      "\n",
      "latent_dim, optimizer, encoder_dropout, decoder_dropout, batch_size\n",
      "32 Adam 0.5 0.3 64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-e17b2cfa1f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     )\n\u001b[1;32m     20\u001b[0m                     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hists = list()\n",
    "for latent_dim in [32, 64, 128]:\n",
    "    for optimizer in ['Adam']:\n",
    "        for encoder_dropout in [0.3, 0.5]:\n",
    "            for decoder_dropout in [0.3, 0.5]:\n",
    "                for batch_size in [64, 128]:\n",
    "                    print()\n",
    "                    print(\"latent_dim, optimizer, encoder_dropout, decoder_dropout, batch_size\")\n",
    "                    print(latent_dim, optimizer, encoder_dropout, decoder_dropout, batch_size)\n",
    "                    model = build_model(latent_dim, optimizer, encoder_dropout, decoder_dropout)\n",
    "                    hist = model.fit(\n",
    "                        [encoder_input_data, decoder_input_data],\n",
    "                        decoder_target_data,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=0.15,\n",
    "                        callbacks=[earlyStopping],\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    m = max(hist.history[\"val_acc\"])\n",
    "                    print(\"best val_acc:\", m)\n",
    "                    print(\"on epoch\", hist.history[\"val_acc\"].index(m))\n",
    "                    hists.append(hist)\n",
    "                \n",
    "with open(\"hists.pickle\", \"wb\") as f:\n",
    "    pickle.dump(hists, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10445 samples, validate on 2612 samples\n",
      "Epoch 1/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 1.9401 - acc: 0.5319 - val_loss: 1.5520 - val_acc: 0.5617\n",
      "Epoch 2/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 1.5187 - acc: 0.5682 - val_loss: 1.5092 - val_acc: 0.5761\n",
      "Epoch 3/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 1.4295 - acc: 0.5939 - val_loss: 1.3715 - val_acc: 0.6171\n",
      "Epoch 4/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 1.3262 - acc: 0.6273 - val_loss: 1.2786 - val_acc: 0.6458\n",
      "Epoch 5/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 1.2236 - acc: 0.6568 - val_loss: 1.1582 - val_acc: 0.6770\n",
      "Epoch 6/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 1.1370 - acc: 0.6785 - val_loss: 1.0733 - val_acc: 0.6992\n",
      "Epoch 7/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 1.0707 - acc: 0.6955 - val_loss: 1.0304 - val_acc: 0.7098\n",
      "Epoch 8/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 1.0268 - acc: 0.7061 - val_loss: 0.9738 - val_acc: 0.7241\n",
      "Epoch 9/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.9873 - acc: 0.7156 - val_loss: 0.9329 - val_acc: 0.7328\n",
      "Epoch 10/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.9577 - acc: 0.7250 - val_loss: 0.9059 - val_acc: 0.7419\n",
      "Epoch 11/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.9296 - acc: 0.7326 - val_loss: 0.8749 - val_acc: 0.7485\n",
      "Epoch 12/100\n",
      "10445/10445 [==============================] - 16s 1ms/step - loss: 0.9036 - acc: 0.7397 - val_loss: 0.8558 - val_acc: 0.7560\n",
      "Epoch 13/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.8822 - acc: 0.7465 - val_loss: 0.8342 - val_acc: 0.7643\n",
      "Epoch 14/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.8636 - acc: 0.7510 - val_loss: 0.8088 - val_acc: 0.7691\n",
      "Epoch 15/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.8451 - acc: 0.7575 - val_loss: 0.8005 - val_acc: 0.7716\n",
      "Epoch 16/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.8303 - acc: 0.7610 - val_loss: 0.7862 - val_acc: 0.7794\n",
      "Epoch 17/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.8166 - acc: 0.7652 - val_loss: 0.7675 - val_acc: 0.7798\n",
      "Epoch 18/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.8047 - acc: 0.7695 - val_loss: 0.7620 - val_acc: 0.7836\n",
      "Epoch 19/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.7907 - acc: 0.7730 - val_loss: 0.7574 - val_acc: 0.7854\n",
      "Epoch 20/100\n",
      "10445/10445 [==============================] - 17s 2ms/step - loss: 0.7804 - acc: 0.7751 - val_loss: 0.7452 - val_acc: 0.7883\n",
      "Epoch 21/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.7702 - acc: 0.7786 - val_loss: 0.7309 - val_acc: 0.7922\n",
      "Epoch 22/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.7606 - acc: 0.7813 - val_loss: 0.7231 - val_acc: 0.7942\n",
      "Epoch 23/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.7489 - acc: 0.7847 - val_loss: 0.7073 - val_acc: 0.7990\n",
      "Epoch 24/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.7424 - acc: 0.7855 - val_loss: 0.7167 - val_acc: 0.7990\n",
      "Epoch 25/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.7340 - acc: 0.7898 - val_loss: 0.7044 - val_acc: 0.7999\n",
      "Epoch 26/100\n",
      "10445/10445 [==============================] - 17s 2ms/step - loss: 0.7265 - acc: 0.7908 - val_loss: 0.6944 - val_acc: 0.8018\n",
      "Epoch 27/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.7169 - acc: 0.7933 - val_loss: 0.6936 - val_acc: 0.8039\n",
      "Epoch 28/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.7093 - acc: 0.7964 - val_loss: 0.6754 - val_acc: 0.8077\n",
      "Epoch 29/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.7033 - acc: 0.7985 - val_loss: 0.6747 - val_acc: 0.8064\n",
      "Epoch 30/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.6969 - acc: 0.7998 - val_loss: 0.6707 - val_acc: 0.8097\n",
      "Epoch 31/100\n",
      "10445/10445 [==============================] - 19s 2ms/step - loss: 0.6897 - acc: 0.8017 - val_loss: 0.6651 - val_acc: 0.8110\n",
      "Epoch 32/100\n",
      "10445/10445 [==============================] - 16s 1ms/step - loss: 0.6824 - acc: 0.8046 - val_loss: 0.6630 - val_acc: 0.8140\n",
      "Epoch 33/100\n",
      "10445/10445 [==============================] - 16s 1ms/step - loss: 0.6781 - acc: 0.8051 - val_loss: 0.6619 - val_acc: 0.8151\n",
      "Epoch 34/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.6716 - acc: 0.8069 - val_loss: 0.6524 - val_acc: 0.8156\n",
      "Epoch 35/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.6639 - acc: 0.8096 - val_loss: 0.6479 - val_acc: 0.8177\n",
      "Epoch 36/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.6583 - acc: 0.8112 - val_loss: 0.6409 - val_acc: 0.8201\n",
      "Epoch 37/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.6547 - acc: 0.8112 - val_loss: 0.6370 - val_acc: 0.8197\n",
      "Epoch 38/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.6471 - acc: 0.8143 - val_loss: 0.6380 - val_acc: 0.8192\n",
      "Epoch 39/100\n",
      "10445/10445 [==============================] - 16s 1ms/step - loss: 0.6416 - acc: 0.8156 - val_loss: 0.6339 - val_acc: 0.8206\n",
      "Epoch 40/100\n",
      "10445/10445 [==============================] - 18s 2ms/step - loss: 0.6364 - acc: 0.8169 - val_loss: 0.6352 - val_acc: 0.8208\n",
      "Epoch 41/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.6326 - acc: 0.8181 - val_loss: 0.6308 - val_acc: 0.8221\n",
      "Epoch 42/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.6251 - acc: 0.8200 - val_loss: 0.6241 - val_acc: 0.8248\n",
      "Epoch 43/100\n",
      "10445/10445 [==============================] - 17s 2ms/step - loss: 0.6228 - acc: 0.8207 - val_loss: 0.6185 - val_acc: 0.8246\n",
      "Epoch 44/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 0.6182 - acc: 0.8232 - val_loss: 0.6188 - val_acc: 0.8256\n",
      "Epoch 45/100\n",
      "10445/10445 [==============================] - 16s 1ms/step - loss: 0.6127 - acc: 0.8244 - val_loss: 0.6210 - val_acc: 0.8252\n",
      "Epoch 46/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 0.6060 - acc: 0.8263 - val_loss: 0.6154 - val_acc: 0.8263\n",
      "Epoch 47/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.6038 - acc: 0.8267 - val_loss: 0.6104 - val_acc: 0.8285\n",
      "Epoch 48/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 0.6017 - acc: 0.8270 - val_loss: 0.6082 - val_acc: 0.8304\n",
      "Epoch 49/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 0.5943 - acc: 0.8295 - val_loss: 0.6090 - val_acc: 0.8283\n",
      "Epoch 50/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5913 - acc: 0.8301 - val_loss: 0.6055 - val_acc: 0.8289\n",
      "Epoch 51/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5857 - acc: 0.8315 - val_loss: 0.6061 - val_acc: 0.8284\n",
      "Epoch 52/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5812 - acc: 0.8328 - val_loss: 0.6022 - val_acc: 0.8318\n",
      "Epoch 53/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5773 - acc: 0.8336 - val_loss: 0.5974 - val_acc: 0.8326\n",
      "Epoch 54/100\n",
      "10445/10445 [==============================] - 18s 2ms/step - loss: 0.5721 - acc: 0.8357 - val_loss: 0.6078 - val_acc: 0.8296\n",
      "Epoch 55/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.5684 - acc: 0.8376 - val_loss: 0.5987 - val_acc: 0.8316\n",
      "Epoch 56/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 0.5656 - acc: 0.8371 - val_loss: 0.5890 - val_acc: 0.8340\n",
      "Epoch 57/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5595 - acc: 0.8390 - val_loss: 0.5973 - val_acc: 0.8304\n",
      "Epoch 58/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5568 - acc: 0.8399 - val_loss: 0.5906 - val_acc: 0.8344\n",
      "Epoch 59/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.5528 - acc: 0.8411 - val_loss: 0.5941 - val_acc: 0.8346\n",
      "Epoch 60/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.5508 - acc: 0.8414 - val_loss: 0.5918 - val_acc: 0.8338\n",
      "Epoch 61/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5451 - acc: 0.8423 - val_loss: 0.5828 - val_acc: 0.8362\n",
      "Epoch 62/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5410 - acc: 0.8450 - val_loss: 0.5805 - val_acc: 0.8363\n",
      "Epoch 63/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5374 - acc: 0.8457 - val_loss: 0.5825 - val_acc: 0.8379\n",
      "Epoch 64/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.5344 - acc: 0.8461 - val_loss: 0.5825 - val_acc: 0.8380\n",
      "Epoch 65/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.5301 - acc: 0.8482 - val_loss: 0.5856 - val_acc: 0.8363\n",
      "Epoch 66/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 0.5272 - acc: 0.8484 - val_loss: 0.5807 - val_acc: 0.8374\n",
      "Epoch 67/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.5236 - acc: 0.8494 - val_loss: 0.5762 - val_acc: 0.8394\n",
      "Epoch 68/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.5188 - acc: 0.8509 - val_loss: 0.5773 - val_acc: 0.8382\n",
      "Epoch 69/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 0.5149 - acc: 0.8523 - val_loss: 0.5855 - val_acc: 0.8370\n",
      "Epoch 70/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5145 - acc: 0.8528 - val_loss: 0.5766 - val_acc: 0.8381\n",
      "Epoch 71/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5083 - acc: 0.8536 - val_loss: 0.5677 - val_acc: 0.8421\n",
      "Epoch 72/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5076 - acc: 0.8536 - val_loss: 0.5796 - val_acc: 0.8381\n",
      "Epoch 73/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.5033 - acc: 0.8550 - val_loss: 0.5742 - val_acc: 0.8388\n",
      "Epoch 74/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4977 - acc: 0.8567 - val_loss: 0.5700 - val_acc: 0.8409\n",
      "Epoch 75/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4965 - acc: 0.8572 - val_loss: 0.5709 - val_acc: 0.8403\n",
      "Epoch 76/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4920 - acc: 0.8583 - val_loss: 0.5729 - val_acc: 0.8412\n",
      "Epoch 77/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4903 - acc: 0.8581 - val_loss: 0.5713 - val_acc: 0.8420\n",
      "Epoch 78/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4857 - acc: 0.8604 - val_loss: 0.5690 - val_acc: 0.8405\n",
      "Epoch 79/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4808 - acc: 0.8617 - val_loss: 0.5656 - val_acc: 0.8416\n",
      "Epoch 80/100\n",
      "10445/10445 [==============================] - 18s 2ms/step - loss: 0.4784 - acc: 0.8614 - val_loss: 0.5673 - val_acc: 0.8422\n",
      "Epoch 81/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.4777 - acc: 0.8623 - val_loss: 0.5642 - val_acc: 0.8430\n",
      "Epoch 82/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.4755 - acc: 0.8626 - val_loss: 0.5685 - val_acc: 0.8411\n",
      "Epoch 83/100\n",
      "10445/10445 [==============================] - 14s 1ms/step - loss: 0.4710 - acc: 0.8649 - val_loss: 0.5670 - val_acc: 0.8432\n",
      "Epoch 84/100\n",
      "10445/10445 [==============================] - 17s 2ms/step - loss: 0.4664 - acc: 0.8657 - val_loss: 0.5617 - val_acc: 0.8447\n",
      "Epoch 85/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4629 - acc: 0.8660 - val_loss: 0.5657 - val_acc: 0.8436\n",
      "Epoch 86/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4629 - acc: 0.8670 - val_loss: 0.5632 - val_acc: 0.8438\n",
      "Epoch 87/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4585 - acc: 0.8678 - val_loss: 0.5676 - val_acc: 0.8436\n",
      "Epoch 88/100\n",
      "10445/10445 [==============================] - 19s 2ms/step - loss: 0.4555 - acc: 0.8682 - val_loss: 0.5610 - val_acc: 0.8445\n",
      "Epoch 89/100\n",
      "10445/10445 [==============================] - 17s 2ms/step - loss: 0.4505 - acc: 0.8697 - val_loss: 0.5622 - val_acc: 0.8431\n",
      "Epoch 90/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4514 - acc: 0.8696 - val_loss: 0.5632 - val_acc: 0.8440\n",
      "Epoch 91/100\n",
      "10445/10445 [==============================] - 16s 1ms/step - loss: 0.4468 - acc: 0.8713 - val_loss: 0.5626 - val_acc: 0.8439\n",
      "Epoch 92/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4437 - acc: 0.8719 - val_loss: 0.5575 - val_acc: 0.8457\n",
      "Epoch 93/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4418 - acc: 0.8723 - val_loss: 0.5581 - val_acc: 0.8470\n",
      "Epoch 94/100\n",
      "10445/10445 [==============================] - 16s 2ms/step - loss: 0.4378 - acc: 0.8737 - val_loss: 0.5628 - val_acc: 0.8447\n",
      "Epoch 95/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4353 - acc: 0.8737 - val_loss: 0.5649 - val_acc: 0.8433\n",
      "Epoch 96/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4332 - acc: 0.8746 - val_loss: 0.5593 - val_acc: 0.8446\n",
      "Epoch 97/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4293 - acc: 0.8757 - val_loss: 0.5620 - val_acc: 0.8458\n",
      "Epoch 98/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4279 - acc: 0.8767 - val_loss: 0.5626 - val_acc: 0.8452\n",
      "Epoch 99/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4270 - acc: 0.8762 - val_loss: 0.5586 - val_acc: 0.8456\n",
      "Epoch 100/100\n",
      "10445/10445 [==============================] - 15s 1ms/step - loss: 0.4222 - acc: 0.8777 - val_loss: 0.5600 - val_acc: 0.8462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_3/concat:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'concatenate_4/concat:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM-128\n",
    "\n",
    "latent_dim = 128\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = Bidirectional(LSTM(latent_dim, return_state=True, dropout=0.5))\n",
    "\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True, dropout=0.1)\n",
    "# decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=3e-4)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,\n",
    "         callbacks=[earlyStopping])\n",
    "\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention LSTM\n",
    "# Code from\n",
    "# https://medium.com/@jbetker/implementing-seq2seq-with-attention-in-keras-63565c8e498c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'lstm_with_attention_1/transpose_1:0' shape=(?, ?, ?) dtype=float32>,\n",
       " <tf.Tensor 'lstm_with_attention_1/while/Exit_2:0' shape=(?, 128) dtype=float32>,\n",
       " <tf.Tensor 'lstm_with_attention_1/while/Exit_3:0' shape=(?, 128) dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN \"Cell\" classes in Keras perform the actual data transformations at each timestep. Therefore, in order\n",
    "# to add attention to LSTM, we need to make a custom subclass of LSTMCell.\n",
    "class AttentionLSTMCell(LSTMCell):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.attentionMode = False\n",
    "        super(AttentionLSTMCell, self).__init__(**kwargs)\n",
    "    \n",
    "    # Build is called to initialize the variables that our cell will use. We will let other Keras\n",
    "    # classes (e.g. \"Dense\") actually initialize these variables.\n",
    "    @tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):        \n",
    "        # Converts the input sequence into a sequence which can be matched up to the internal\n",
    "        # hidden state.\n",
    "        self.dense_constant = TimeDistributed(Dense(self.units, name=\"AttLstmInternal_DenseConstant\"))\n",
    "        \n",
    "        # Transforms the internal hidden state into something that can be used by the attention\n",
    "        # mechanism.\n",
    "        self.dense_state = Dense(self.units, name=\"AttLstmInternal_DenseState\")\n",
    "        \n",
    "        # Transforms the combined hidden state and converted input sequence into a vector of\n",
    "        # probabilities for attention.\n",
    "        self.dense_transform = Dense(1, name=\"AttLstmInternal_DenseTransform\")\n",
    "        \n",
    "        # We will augment the input into LSTMCell by concatenating the context vector. Modify\n",
    "        # input_shape to reflect this.\n",
    "        batch, input_dim = input_shape[0]\n",
    "        batch, timesteps, context_size = input_shape[-1]\n",
    "        lstm_input = (batch, input_dim + context_size)\n",
    "        \n",
    "        # The LSTMCell superclass expects no constant input, so strip that out.\n",
    "        return super(AttentionLSTMCell, self).build(lstm_input)\n",
    "    \n",
    "    # This must be called before call(). The \"input sequence\" is the output from the \n",
    "    # encoder. This function will do some pre-processing on that sequence which will\n",
    "    # then be used in subsequent calls.\n",
    "    def setInputSequence(self, input_seq):\n",
    "        self.input_seq = input_seq\n",
    "        self.input_seq_shaped = self.dense_constant(input_seq)\n",
    "        self.timesteps = tf.shape(self.input_seq)[-2]\n",
    "    \n",
    "    # This is a utility method to adjust the output of this cell. When attention mode is\n",
    "    # turned on, the cell outputs attention probability vectors across the input sequence.\n",
    "    def setAttentionMode(self, mode_on=False):\n",
    "        self.attentionMode = mode_on\n",
    "    \n",
    "    # This method sets up the computational graph for the cell. It implements the actual logic\n",
    "    # that the model follows.\n",
    "    def call(self, inputs, states, constants):\n",
    "        # Separate the state list into the two discrete state vectors.\n",
    "        # ytm is the \"memory state\", stm is the \"carry state\".\n",
    "        ytm, stm = states\n",
    "        # We will use the \"carry state\" to guide the attention mechanism. Repeat it across all\n",
    "        # input timesteps to perform some calculations on it.\n",
    "        stm_repeated = K.repeat(self.dense_state(stm), self.timesteps)\n",
    "        # Now apply our \"dense_transform\" operation on the sum of our transformed \"carry state\" \n",
    "        # and all encoder states. This will squash the resultant sum down to a vector of size\n",
    "        # [batch,timesteps,1]\n",
    "        # Note: Most sources I encounter use tanh for the activation here. I have found with this dataset\n",
    "        # and this model, relu seems to perform better. It makes the attention mechanism far more crisp\n",
    "        # and produces better translation performance, especially with respect to proper sentence termination.\n",
    "        combined_stm_input = self.dense_transform(\n",
    "            keras.activations.relu(stm_repeated + self.input_seq_shaped))\n",
    "        # Performing a softmax generates a log probability for each encoder output to receive attention.\n",
    "        score_vector = keras.activations.softmax(combined_stm_input, 1)\n",
    "        # In this implementation, we grant \"partial attention\" to each encoder output based on \n",
    "        # it's log probability accumulated above. Other options would be to only give attention\n",
    "        # to the highest probability encoder output or some similar set.\n",
    "        context_vector = K.sum(score_vector * self.input_seq, 1)\n",
    "        \n",
    "        # Finally, mutate the input vector. It will now contain the traditional inputs (like the seq2seq\n",
    "        # we trained above) in addition to the attention context vector we calculated earlier in this method.\n",
    "        inputs = K.concatenate([inputs, context_vector])\n",
    "        \n",
    "        # Call into the super-class to invoke the LSTM math.\n",
    "        res = super(AttentionLSTMCell, self).call(inputs=inputs, states=states)\n",
    "        \n",
    "        # This if statement switches the return value of this method if \"attentionMode\" is turned on.\n",
    "        if(self.attentionMode):\n",
    "            return (K.reshape(score_vector, (-1, self.timesteps)), res[1])\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "# Custom implementation of the Keras LSTM that adds an attention mechanism.\n",
    "# This is implemented by taking an additional input (using the \"constants\" of the\n",
    "# RNN class) into the LSTM: The encoder output vectors across the entire input sequence.\n",
    "class LSTMWithAttention(RNN):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        cell = AttentionLSTMCell(units=units)\n",
    "        self.units = units\n",
    "        super(LSTMWithAttention, self).__init__(cell, **kwargs)\n",
    "        \n",
    "    @tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[0][-1]\n",
    "        self.timesteps = input_shape[0][-2]\n",
    "        return super(LSTMWithAttention, self).build(input_shape) \n",
    "    \n",
    "    # This call is invoked with the entire time sequence. The RNN sub-class is responsible\n",
    "    # for breaking this up into calls into the cell for each step.\n",
    "    # The \"constants\" variable is the key to our implementation. It was specifically added\n",
    "    # to Keras to accomodate the \"attention\" mechanism we are implementing.\n",
    "    def call(self, x, constants, **kwargs):\n",
    "        if isinstance(x, list):\n",
    "            self.x_initial = x[0]\n",
    "        else:\n",
    "            self.x_initial = x\n",
    "        \n",
    "        # The only difference in the LSTM computational graph really comes from the custom\n",
    "        # LSTM Cell that we utilize.\n",
    "        self.cell._dropout_mask = None\n",
    "        self.cell._recurrent_dropout_mask = None\n",
    "        self.cell.setInputSequence(constants[0])\n",
    "        return super(LSTMWithAttention, self).call(inputs=x, constants=constants, **kwargs)\n",
    "\n",
    "# Below is test code to validate that this LSTM class and the associated cell create a\n",
    "# valid computational graph.\n",
    "test = LSTMWithAttention(units=latent_dim, return_sequences=True, return_state=True)\n",
    "test.cell.setAttentionMode(True)\n",
    "# attenc_inputs2 = Input(shape=(max_encoder_seq_length,))\n",
    "# attenc_emb2 = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "attenc_emb2 = Input(shape=(None, num_encoder_tokens))\n",
    "test(inputs=attenc_emb2, constants=attenc_emb2, initial_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attention_model(latent_dim, optimizer, encoder_dropout, decoder_dropout):\n",
    "    attenc_inputs = Input(shape=(max_encoder_seq_length, num_encoder_tokens), name=\"attenc_inputs\")\n",
    "    attenc_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True, recurrent_dropout=encoder_dropout))\n",
    "    attenc_outputs, forward_h, forward_c, backward_h, backward_c = attenc_lstm(attenc_inputs)\n",
    "    \n",
    "    attstate_h = Concatenate()([forward_h, backward_h])\n",
    "    attstate_c = Concatenate()([forward_c, backward_c])\n",
    "    attenc_states = [attstate_h, attstate_c]\n",
    "\n",
    "    attdec_inputs = Input(shape=(max_decoder_seq_length, num_decoder_tokens))\n",
    "    attdec_lstm = LSTMWithAttention(units=latent_dim*2, return_sequences=True, return_state=True)\n",
    "    attdec_lstm_out, _, _ = attdec_lstm(inputs=attdec_inputs, \n",
    "                                        constants=attenc_outputs, \n",
    "                                        initial_state=attenc_states)\n",
    "    attdec_d1 = Dense(latent_dim, activation=\"relu\")\n",
    "    attdec_d2 = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "    attdec_out = attdec_d2(Dropout(rate=decoder_dropout)(attdec_d1(Dropout(rate=decoder_dropout)(attdec_lstm_out))))\n",
    "\n",
    "    attmodel = Model([attenc_inputs, attdec_inputs], attdec_out)\n",
    "    attmodel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['categorical_accuracy'])\n",
    "    return attmodel\n",
    "\n",
    "model = build_attention_model(64, keras.optimizers.adam(lr=5e-3), 0.5, 0.4)\n",
    "hist = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[earlyStopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = list()\n",
    "for latent_dim in [32, 64, 128]:\n",
    "    for optimizer in ['Adam', 'rmsprop']:\n",
    "        for encoder_dropout in [0.1, 0.3, 0.5]:\n",
    "            for decoder_dropout in [0,1, 0.3, 0.5]:\n",
    "                for batch_size in [64, 128]:\n",
    "                print()\n",
    "                print(\"latent_dim, optimizer, encoder_dropout, decoder_dropout, batch_size\")\n",
    "                print(latent_dim, optimizer, encoder_dropout, decoder_dropout, batch_size)\n",
    "                model = build_attention_model(latent_dim, optimizer, encoder_dropout, decoder_dropout)\n",
    "                hist = model.fit(\n",
    "                    [encoder_input_data, decoder_input_data],\n",
    "                    decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.15,\n",
    "                    callbacks=[earlyStopping],\n",
    "                    verbose=0\n",
    "                )\n",
    "                m = max(hist.history[\"val_acc\"])\n",
    "                print(\"best val_acc:\", m)\n",
    "                print(\"on epoch\", hist.history[\"val_acc\"].index(m))\n",
    "                hists.append(hist)\n",
    "                \n",
    "with open(\"hists_att.pickle\", \"wb\") as f:\n",
    "    pickle.dump(hists, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10445 samples, validate on 2612 samples\n",
      "Epoch 1/100\n",
      "10445/10445 [==============================] - 31s 3ms/step - loss: 1.3502 - acc: 0.6221 - val_loss: 0.9706 - val_acc: 0.7173\n",
      "Epoch 2/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 1.0141 - acc: 0.7053 - val_loss: 0.8408 - val_acc: 0.7628\n",
      "Epoch 3/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.9352 - acc: 0.7299 - val_loss: 0.7885 - val_acc: 0.7808\n",
      "Epoch 4/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.8735 - acc: 0.7491 - val_loss: 0.7153 - val_acc: 0.7970\n",
      "Epoch 5/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.8218 - acc: 0.7644 - val_loss: 0.6864 - val_acc: 0.8044\n",
      "Epoch 6/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.7819 - acc: 0.7771 - val_loss: 0.6492 - val_acc: 0.8158\n",
      "Epoch 7/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.7471 - acc: 0.7878 - val_loss: 0.6252 - val_acc: 0.8222\n",
      "Epoch 8/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.7161 - acc: 0.7968 - val_loss: 0.6044 - val_acc: 0.8305\n",
      "Epoch 9/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.6902 - acc: 0.8050 - val_loss: 0.5919 - val_acc: 0.8349\n",
      "Epoch 10/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.6667 - acc: 0.8131 - val_loss: 0.5813 - val_acc: 0.8363\n",
      "Epoch 11/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.6428 - acc: 0.8188 - val_loss: 0.5744 - val_acc: 0.8389\n",
      "Epoch 12/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.6246 - acc: 0.8256 - val_loss: 0.5566 - val_acc: 0.8456\n",
      "Epoch 13/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.6068 - acc: 0.8290 - val_loss: 0.5541 - val_acc: 0.8460\n",
      "Epoch 14/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.5915 - acc: 0.8336 - val_loss: 0.5503 - val_acc: 0.8500\n",
      "Epoch 15/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.5758 - acc: 0.8379 - val_loss: 0.5431 - val_acc: 0.8507\n",
      "Epoch 16/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.5641 - acc: 0.8415 - val_loss: 0.5447 - val_acc: 0.8524\n",
      "Epoch 17/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.5462 - acc: 0.8469 - val_loss: 0.5343 - val_acc: 0.8553\n",
      "Epoch 18/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.5374 - acc: 0.8491 - val_loss: 0.5335 - val_acc: 0.8536\n",
      "Epoch 19/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.5263 - acc: 0.8522 - val_loss: 0.5351 - val_acc: 0.8555\n",
      "Epoch 20/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.5146 - acc: 0.8552 - val_loss: 0.5384 - val_acc: 0.8550\n",
      "Epoch 21/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.5060 - acc: 0.8570 - val_loss: 0.5407 - val_acc: 0.8554\n",
      "Epoch 22/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4983 - acc: 0.8592 - val_loss: 0.5311 - val_acc: 0.8586\n",
      "Epoch 23/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4885 - acc: 0.8630 - val_loss: 0.5322 - val_acc: 0.8589\n",
      "Epoch 24/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4786 - acc: 0.8648 - val_loss: 0.5298 - val_acc: 0.8600\n",
      "Epoch 25/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4700 - acc: 0.8674 - val_loss: 0.5290 - val_acc: 0.8610\n",
      "Epoch 26/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4637 - acc: 0.8695 - val_loss: 0.5336 - val_acc: 0.8602\n",
      "Epoch 27/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4568 - acc: 0.8710 - val_loss: 0.5323 - val_acc: 0.8619\n",
      "Epoch 28/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4473 - acc: 0.8744 - val_loss: 0.5455 - val_acc: 0.8609\n",
      "Epoch 29/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4447 - acc: 0.8742 - val_loss: 0.5493 - val_acc: 0.8606\n",
      "Epoch 30/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4353 - acc: 0.8764 - val_loss: 0.5436 - val_acc: 0.8611\n",
      "Epoch 31/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4309 - acc: 0.8776 - val_loss: 0.5401 - val_acc: 0.8596\n",
      "Epoch 32/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4257 - acc: 0.8795 - val_loss: 0.5383 - val_acc: 0.8611\n",
      "Epoch 33/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4199 - acc: 0.8810 - val_loss: 0.5462 - val_acc: 0.8617\n",
      "Epoch 34/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4165 - acc: 0.8818 - val_loss: 0.5462 - val_acc: 0.8624\n",
      "Epoch 35/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4105 - acc: 0.8839 - val_loss: 0.5466 - val_acc: 0.8634\n",
      "Epoch 36/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4067 - acc: 0.8855 - val_loss: 0.5572 - val_acc: 0.8610\n",
      "Epoch 37/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.4016 - acc: 0.8862 - val_loss: 0.5579 - val_acc: 0.8629\n",
      "Epoch 38/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3973 - acc: 0.8875 - val_loss: 0.5612 - val_acc: 0.8621\n",
      "Epoch 39/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3925 - acc: 0.8885 - val_loss: 0.5585 - val_acc: 0.8633\n",
      "Epoch 40/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3894 - acc: 0.8897 - val_loss: 0.5597 - val_acc: 0.8627\n",
      "Epoch 41/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3870 - acc: 0.8899 - val_loss: 0.5619 - val_acc: 0.8645\n",
      "Epoch 42/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3862 - acc: 0.8902 - val_loss: 0.5575 - val_acc: 0.8639\n",
      "Epoch 43/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3790 - acc: 0.8920 - val_loss: 0.5515 - val_acc: 0.8642\n",
      "Epoch 44/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3733 - acc: 0.8934 - val_loss: 0.5763 - val_acc: 0.8638\n",
      "Epoch 45/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3698 - acc: 0.8950 - val_loss: 0.5608 - val_acc: 0.8640\n",
      "Epoch 46/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3711 - acc: 0.8949 - val_loss: 0.5690 - val_acc: 0.8631\n",
      "Epoch 47/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3681 - acc: 0.8952 - val_loss: 0.5628 - val_acc: 0.8634\n",
      "Epoch 48/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3643 - acc: 0.8963 - val_loss: 0.5788 - val_acc: 0.8627\n",
      "Epoch 49/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3621 - acc: 0.8964 - val_loss: 0.5659 - val_acc: 0.8636\n",
      "Epoch 50/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3598 - acc: 0.8971 - val_loss: 0.5703 - val_acc: 0.8624\n",
      "Epoch 51/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3566 - acc: 0.8984 - val_loss: 0.5875 - val_acc: 0.8625\n",
      "Epoch 52/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3530 - acc: 0.8995 - val_loss: 0.5857 - val_acc: 0.8625\n",
      "Epoch 53/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3485 - acc: 0.9008 - val_loss: 0.5718 - val_acc: 0.8637\n",
      "Epoch 54/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3481 - acc: 0.9010 - val_loss: 0.5801 - val_acc: 0.8643\n",
      "Epoch 55/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3470 - acc: 0.9003 - val_loss: 0.5830 - val_acc: 0.8615\n",
      "Epoch 56/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3468 - acc: 0.9008 - val_loss: 0.5861 - val_acc: 0.8635\n",
      "Epoch 57/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3407 - acc: 0.9026 - val_loss: 0.6047 - val_acc: 0.8624\n",
      "Epoch 58/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3399 - acc: 0.9031 - val_loss: 0.6076 - val_acc: 0.8629\n",
      "Epoch 59/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3368 - acc: 0.9036 - val_loss: 0.6002 - val_acc: 0.8614\n",
      "Epoch 60/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3342 - acc: 0.9046 - val_loss: 0.5832 - val_acc: 0.8634\n",
      "Epoch 61/100\n",
      "10445/10445 [==============================] - 25s 2ms/step - loss: 0.3319 - acc: 0.9052 - val_loss: 0.5849 - val_acc: 0.8636\n",
      "Epoch 62/100\n",
      "10445/10445 [==============================] - 26s 2ms/step - loss: 0.3308 - acc: 0.9053 - val_loss: 0.5923 - val_acc: 0.8642\n",
      "Epoch 63/100\n",
      "10445/10445 [==============================] - 29s 3ms/step - loss: 0.3272 - acc: 0.9065 - val_loss: 0.5930 - val_acc: 0.8642\n",
      "Epoch 64/100\n",
      "10445/10445 [==============================] - 29s 3ms/step - loss: 0.3307 - acc: 0.9059 - val_loss: 0.6083 - val_acc: 0.8628\n",
      "Epoch 65/100\n",
      "10445/10445 [==============================] - 31s 3ms/step - loss: 0.3255 - acc: 0.9069 - val_loss: 0.6146 - val_acc: 0.8640\n",
      "Epoch 66/100\n",
      "10445/10445 [==============================] - 30s 3ms/step - loss: 0.3233 - acc: 0.9073 - val_loss: 0.6018 - val_acc: 0.8633\n",
      "Epoch 67/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.3244 - acc: 0.9076 - val_loss: 0.6031 - val_acc: 0.8624\n",
      "Epoch 68/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.3254 - acc: 0.9066 - val_loss: 0.6138 - val_acc: 0.8634\n",
      "Epoch 69/100\n",
      "10445/10445 [==============================] - 29s 3ms/step - loss: 0.3161 - acc: 0.9094 - val_loss: 0.6179 - val_acc: 0.8628\n",
      "Epoch 70/100\n",
      "10445/10445 [==============================] - 29s 3ms/step - loss: 0.3174 - acc: 0.9096 - val_loss: 0.6192 - val_acc: 0.8636\n",
      "Epoch 71/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.3184 - acc: 0.9087 - val_loss: 0.6168 - val_acc: 0.8634\n",
      "Epoch 72/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.3127 - acc: 0.9103 - val_loss: 0.6214 - val_acc: 0.8640\n",
      "Epoch 73/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.3149 - acc: 0.9097 - val_loss: 0.6178 - val_acc: 0.8640\n",
      "Epoch 74/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.3116 - acc: 0.9106 - val_loss: 0.6130 - val_acc: 0.8657\n",
      "Epoch 75/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.3082 - acc: 0.9119 - val_loss: 0.6228 - val_acc: 0.8619\n",
      "Epoch 76/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.3083 - acc: 0.9124 - val_loss: 0.6304 - val_acc: 0.8623\n",
      "Epoch 77/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.3091 - acc: 0.9118 - val_loss: 0.6352 - val_acc: 0.8629\n",
      "Epoch 78/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.3099 - acc: 0.9111 - val_loss: 0.6157 - val_acc: 0.8649\n",
      "Epoch 79/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.3063 - acc: 0.9118 - val_loss: 0.6285 - val_acc: 0.8631\n",
      "Epoch 80/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.3018 - acc: 0.9136 - val_loss: 0.6260 - val_acc: 0.8638\n",
      "Epoch 81/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.3020 - acc: 0.9134 - val_loss: 0.6171 - val_acc: 0.8643\n",
      "Epoch 82/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.2981 - acc: 0.9153 - val_loss: 0.6280 - val_acc: 0.8646\n",
      "Epoch 83/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2978 - acc: 0.9143 - val_loss: 0.6328 - val_acc: 0.8643\n",
      "Epoch 84/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2956 - acc: 0.9157 - val_loss: 0.6473 - val_acc: 0.8615\n",
      "Epoch 85/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2966 - acc: 0.9148 - val_loss: 0.6332 - val_acc: 0.8621\n",
      "Epoch 86/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.2970 - acc: 0.9150 - val_loss: 0.6416 - val_acc: 0.8640\n",
      "Epoch 87/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2949 - acc: 0.9154 - val_loss: 0.6484 - val_acc: 0.8637\n",
      "Epoch 88/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.2947 - acc: 0.9159 - val_loss: 0.6460 - val_acc: 0.8646\n",
      "Epoch 89/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2937 - acc: 0.9159 - val_loss: 0.6438 - val_acc: 0.8631\n",
      "Epoch 90/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2904 - acc: 0.9168 - val_loss: 0.6507 - val_acc: 0.8631\n",
      "Epoch 91/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.2901 - acc: 0.9171 - val_loss: 0.6670 - val_acc: 0.8614\n",
      "Epoch 92/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2876 - acc: 0.9179 - val_loss: 0.6377 - val_acc: 0.8628\n",
      "Epoch 93/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2880 - acc: 0.9175 - val_loss: 0.6472 - val_acc: 0.8627\n",
      "Epoch 94/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2859 - acc: 0.9177 - val_loss: 0.6588 - val_acc: 0.8631\n",
      "Epoch 95/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.2846 - acc: 0.9184 - val_loss: 0.6529 - val_acc: 0.8621\n",
      "Epoch 96/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2846 - acc: 0.9182 - val_loss: 0.6500 - val_acc: 0.8626\n",
      "Epoch 97/100\n",
      "10445/10445 [==============================] - 27s 3ms/step - loss: 0.2832 - acc: 0.9194 - val_loss: 0.6534 - val_acc: 0.8622\n",
      "Epoch 98/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.2836 - acc: 0.9188 - val_loss: 0.6495 - val_acc: 0.8630\n",
      "Epoch 99/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.2823 - acc: 0.9193 - val_loss: 0.6626 - val_acc: 0.8620\n",
      "Epoch 100/100\n",
      "10445/10445 [==============================] - 28s 3ms/step - loss: 0.2774 - acc: 0.9203 - val_loss: 0.6446 - val_acc: 0.8648\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM-128 + Att\n",
    "\n",
    "latent_dim, optimizer, encoder_dropout, decoder_dropout = 128, keras.optimizers.adam(lr=5e-3), 0.4, 0.5\n",
    "attenc_inputs = Input(shape=(max_encoder_seq_length, num_encoder_tokens), name=\"attenc_inputs\")\n",
    "attenc_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, return_state=True, dropout=encoder_dropout, recurrent_dropout=0.2))\n",
    "attenc_outputs, forward_h, forward_c, backward_h, backward_c = attenc_lstm(attenc_inputs)\n",
    "\n",
    "attstate_h = Concatenate()([forward_h, backward_h])\n",
    "attstate_c = Concatenate()([forward_c, backward_c])\n",
    "attenc_states = [attstate_h, attstate_c]\n",
    "\n",
    "attdec_inputs = Input(shape=(max_decoder_seq_length, num_decoder_tokens))\n",
    "attdec_lstm = LSTMWithAttention(units=latent_dim*2, return_sequences=True, return_state=True)\n",
    "attdec_lstm_out, _, _ = attdec_lstm(inputs=attdec_inputs, \n",
    "                                    constants=attenc_outputs, \n",
    "                                    initial_state=attenc_states)\n",
    "attdec_d1 = Dense(latent_dim, activation=\"relu\")\n",
    "attdec_d2 = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "attdec_out = attdec_d2(Dropout(rate=decoder_dropout)(attdec_d1(Dropout(rate=decoder_dropout)(attdec_lstm_out))))\n",
    "\n",
    "attmodel = Model([attenc_inputs, attdec_inputs], attdec_out)\n",
    "attmodel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "hist = attmodel.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.20,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_with_attention_13 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_25/concat:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'concatenate_26/concat:0' shape=(?, 256) dtype=float32>], 'constants': [<tf.Tensor 'bidirectional_13/concat:0' shape=(?, ?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# attmodel.save(\"oldrus_rus_close_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# units = 128\n",
    "# from keras.models import load_model\n",
    "# from keras.utils import CustomObjectScope\n",
    "\n",
    "# with CustomObjectScope({\n",
    "#     \"AttentionLSTMCell\": AttentionLSTMCell,\n",
    "#     \"LSTMWithAttention\": LSTMWithAttention,\n",
    "#     }):\n",
    "#     model = load_model('oldrus_rus_close_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
