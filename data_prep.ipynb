{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"srez.pickle\", \"rb\") as f:\n",
    "    rsrez = pd.DataFrame(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsrez = rsrez[rsrez[\"Часть речи\"].isin([\"ПРИЛ\", \"НАР\", \"СУЩ\", \"\", \"ГЛАГ\", \"МЕСТ\", \"ПРЕД\"]) & (~rsrez[\"Значения\"].isin([\"\", \"?\"]))]\n",
    "\n",
    "rsrez_pos = {\n",
    "    'ПРИЛ': \"adjective\",\n",
    "    'НАР': \"adverb\",\n",
    "    'СУЩ': \"noun\",\n",
    "    'ГЛАГ': \"verb\",\n",
    "    'МЕСТ': \"pronoun\",\n",
    "    '': None,\n",
    "    'ПРЕД': \"preposition\"\n",
    "}\n",
    "\n",
    "rsrez = rsrez.assign(\n",
    "    word=rsrez[\"Словарная статья\"].str.lower(),\n",
    "    definition=rsrez[\"Значения\"].map(lambda x: re.sub(r\"\\n\", \" \", x)).map(lambda x: re.sub(r\"[^\\w\\ ]\", \"\", x).strip()),\n",
    "    pos=srez[\"Часть речи\"].map(srez_pos),\n",
    "    sourse=\"sreznevskiy\"\n",
    ")[[\"old_rus_word\", \"definition\", \"pos\", \"sourse\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "or_dict = rsrez[rsrez.definition.map(lambda x: \" \" not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_w2w = rsrez[rsrez.definition.map(lambda x: \" \" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parsed_410_tomes.pickle\", \"rb\") as f:\n",
    "    t410 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "t410 = t410[~t410.definition.astype(str).str.contains(r\"^(?:к|то)\\ \", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t410 = t410[~t410.word.str.lower().isin(rsrez.old_rus_word.values)]\n",
    "\n",
    "t410 = t410[~t410[\"info\"].map(lambda x: \"союз.\" in x if (x == x) else False)]\n",
    "\n",
    "t410 = t410[t410.definition.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps_conv(l):\n",
    "    if l != l:\n",
    "        return None\n",
    "    conv_dict = {\n",
    "        \"с.\": \"noun\",\n",
    "        \"пр.\": \"adjective\",\n",
    "        \"нар.\": \"adverb\",\n",
    "        \"гл.\": \"verb\",\n",
    "        \"мест.\": \"pronoun\"\n",
    "    } \n",
    "    for k, v in conv_dict.items():\n",
    "        if k in l:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def clean_t410_definition(inp_s):\n",
    "    s = inp_s.lower()\n",
    "    s = re.sub(r\"[^\\w\\ ]\", \"\", s)\n",
    "    s = \" \".join([x for x in s.split() if all(y in \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\" for y in x)])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "t410 = t410.assign(\n",
    "    word=t410[\"word\"].str.lower(),\n",
    "    definition=t410[\"definition\"].map(clean_t410_definition),\n",
    "    pos=t410[\"info\"].map(ps_conv),\n",
    "    sourse=\"avanesov\"\n",
    ")[[\"word\", \"definition\", \"pos\", \"sourse\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# t410.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "or_dict = or_dict.append(or_w2w, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "or_dict = or_dict.append(t410[t410.translation.map(lambda x: \" \" not in x)], ignore_index=True)\n",
    "or_dict = or_dict.assign(word=or_dict.word.str.replace(r\"c\", \"с\").map(lambda x: re.sub(r\"(?:ѩ|˫а)\", \"я\", x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "leters = set()\n",
    "for x in or_dict.word.values:\n",
    "    leters.update(set(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"or_dict.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(or_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"or_dict.pickle\", \"rb\") as f:\n",
    "    or_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39329"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(or_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"oed.pickle\", \"rb\") as f:\n",
    "    oed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oed = oed[oed.word.map(lambda x: len(x) > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_oed(s):\n",
    "    if s == s:\n",
    "        pos = {\n",
    "            \"sb.\": \"noun\",\n",
    "            \"adj.\": \"adjective\",\n",
    "            \"adv.\": \"adverb\",\n",
    "            \"prep.\": \"preposition\",\n",
    "            \"pp.\": \"preposition\",\n",
    "            \"pron.\": \"pronoun\",\n",
    "            \"v.\": \"verb\",\n",
    "        }\n",
    "        for k in pos:\n",
    "            if k in s:\n",
    "                return pos[k]\n",
    "\n",
    "oed = oed.assign(pos=oed.pos.map(pos_oed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = re.compile(r\"\\b(?:t(?:he|o)|a)\\ \")\n",
    "oed = oed.assign(definition=oed.definition.map(lambda x: reg.sub(\"\", x)))\n",
    "reg = re.compile(r\"\\b\\s?(?:a(?:s|nd|d[jv])|noun|verb|prep|conj|rel)\\s?\\b\")\n",
    "oed = oed.assign(definition=oed.definition.map(lambda x: reg.sub(\" \", x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oed = oed.assign(source=\"mayhew_skeat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"oe_dict.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(oe_dict, f)\n",
    "# with open(\"oe_dict.pickle\", \"rb\") as f:\n",
    "#     oe_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letters = set()\n",
    "# for word in oe_dict.word.values:\n",
    "#     letters.update(set(word))\n",
    "# letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"oec.pickle\", \"rb\") as f:\n",
    "    oec = pd.DataFrame(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oec = oec[oec.word.map(lambda x: len(x) > 2) & (~oec.word.duplicated()) & oec.definition.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = set(\"ʒ̣̄̆\") - set(\"ʒ\")\n",
    "oec.assign(word=oec.word.map(lambda word: \"\".join(c for c in word if c not in q)))\n",
    "\n",
    "reg = re.compile(r\"(?:\\([\\w\\-]+\\)?|\\(?[\\w\\-n̆]+\\))\")\n",
    "oec = oec.assign(word=oec.word.map(lambda x: reg.sub(\"\", x.lower())))\n",
    "\n",
    "reg = \"[^\\-abcdefghijklmnopqrstuvwxyzæçéðüþāăčēěīōœūǎǒǔǧʒ]\"\n",
    "oec = oec[~oec.word.str.contains(reg)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_oec(s):\n",
    "    if s == s:\n",
    "        pos = {\n",
    "            \"n.\": \"noun\",\n",
    "            \"ger.\": \"verb\",\n",
    "            \"adj.\": \"adjective\",\n",
    "            \"adv.\": \"adverb\",\n",
    "            \"prep.\": \"preposition\",\n",
    "            \"pp.\": \"preposition\",\n",
    "            \"pron.\": \"pronoun\",\n",
    "            \"v.\": \"verb\",\n",
    "        }\n",
    "        for k in pos:\n",
    "            if k in s:\n",
    "                return pos[k]\n",
    "\n",
    "oec = oec.assign(pos=oec.pos.map(pos_oec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = stopwords.words(\"english\") + [\"adj\", \"adv\", \"noun\", \"verb\", \"prep\", \"conj\", \"rel\", \"made\", \"used\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oec_tokenizer(s):\n",
    "    res = s.lower().split(\"\\n\")[0]\n",
    "    res = re.sub(r\"(?:[\\(].+?\\)|[\\[].+?[\\]$]|[\\(].+?$|[\\[].+?$)\", \"\", res)\n",
    "    res = \" \".join(word for word in re.findall(r\"[abcdefghijklmnopqrstuvwxyz]+\", res) if (len(word) > 2) and (word not in stopw))\n",
    "    return res\n",
    "\n",
    "oec = oec.assign(definition=oec.definition.map(oec_tokenizer))\n",
    "# oec.assign(definition=oec.definition.map(oec_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ȝ\" == \"ʒ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ȝ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ʒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "oec = oec.assign(source=\"compendium\")\n",
    "oe_dict = oed.append(oec, ignore_index=True)\n",
    "oe_dict = oe_dict.assign(definition=oe_dict.definition.str.strip())\n",
    "oe_dict = oe_dict.assign(word=oe_dict.word.map(lambda x: x.translate({ord(\"ʒ\"): \"ȝ\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"oe_dict_3.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(oe_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"doe.pickle\", \"rb\") as f:\n",
    "    doe = pd.DataFrame(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doe_prep_word(s):\n",
    "    res = re.sub(r\"(?:ad[jv]|noun|pron|prep|and)\", \"\", s.lower()).strip().split(\", \")[0]\n",
    "    res = res.translate({ord(x): None for x in \"()*,.1234:?\"})\n",
    "    if len(res) < 3:\n",
    "        return None\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "doe = doe.assign(word=doe.word.map(doe_prep_word))\n",
    "doe = doe[doe.word.notna() & (~doe.word.str.contains(\" \"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_doe(s):\n",
    "    if s == s:\n",
    "        pos = {\n",
    "            \"Noun\": \"noun\",\n",
    "            \"Adj.\": \"adjective\",\n",
    "            \"Adv.\": \"adverb\",\n",
    "            \"prep.\": \"preposition\",\n",
    "            \"Pronoun\": \"pronoun\",\n",
    "            \"Vb.\": \"verb\",\n",
    "        }\n",
    "        for k in pos:\n",
    "            if k in s:\n",
    "                return pos[k]\n",
    "\n",
    "doe = doe.assign(pos=doe.pos.map(pos_doe))\n",
    "# doe.assign(pos=doe.pos.map(pos_doe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def doe_tokenizer(s):\n",
    "#     res = s.lower().split(\"\\n\")[0]\n",
    "#     res = re.sub(r\"(?:[\\(].+?\\)|[\\[].+?[\\]$]|[\\(].+?$|[\\[].+?$)\", \"\", res)\n",
    "#     res = \" \".join(word for word in re.findall(r\"[abcdefghijklmnopqrstuvwxyz]+\", res) if (len(word) > 2) and (word not in stopw))\n",
    "#     return res\n",
    "\n",
    "# oec = oec.assign(definition=oec.definition.map(oec_tokenizer))\n",
    "# # oec.assign(definition=oec.definition.map(oec_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "def tag_mystem(text='Текст нужно передать функции в виде строки!'):  \n",
    "    m = Mystem()\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            tagged.append(lemma.lower() + '_' + pos)\n",
    "        except KeyError:\n",
    "            continue # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 'ADJ', 'ADV': 'ADV', 'ADVPRO': 'ADV', 'ANUM': 'ADJ', 'APRO': 'DET', 'COM': 'ADJ', 'CONJ': 'SCONJ', 'INTJ': 'INTJ', 'NONLEX': 'X', 'NUM': 'NUM', 'PART': 'PART', 'PR': 'ADP', 'S': 'NOUN', 'SPRO': 'PRON', 'UNKN': 'X', 'V': 'VERB'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map'\n",
    "\n",
    "mapping = {}\n",
    "r = requests.get(url, stream=True)\n",
    "for pair in r.text.split('\\n'):\n",
    "    pair = re.sub('\\s+', ' ', pair, flags=re.U).split(' ')\n",
    "    if len(pair) > 1:\n",
    "        mapping[pair[0]] = pair[1]\n",
    "\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping[\"APRO\"] = \"ADJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-e18ad2568cbf>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-e18ad2568cbf>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    except KeyError, IndexError:\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def tag_mystem(text='Текст нужно передать функции в виде строки!'):  \n",
    "    m = Mystem()\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            if pos in mapping:\n",
    "                tagged.append(lemma + '_' + mapping[pos]) # здесь мы конвертируем тэги\n",
    "            else:\n",
    "                tagged.append(lemma + '_X') # на случай, если попадется тэг, которого нет в маппинге\n",
    "        except KeyError, IndexError:\n",
    "            continue # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_name = 'models/ruwikiruscorpora_upos_skipgram_300_2_2018.vec.gz'\n",
    "rusvec_model = gensim.models.KeyedVectors.load_word2vec_format(rv_name, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "      <th>pos</th>\n",
       "      <th>sourse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>абидныи</td>\n",
       "      <td>обидный</td>\n",
       "      <td>adjective</td>\n",
       "      <td>sreznevskiy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>аблань</td>\n",
       "      <td>яблоня</td>\n",
       "      <td>noun</td>\n",
       "      <td>sreznevskiy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>абланьныи</td>\n",
       "      <td>яблоневый</td>\n",
       "      <td>adjective</td>\n",
       "      <td>sreznevskiy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>аблъко</td>\n",
       "      <td>яблоко</td>\n",
       "      <td>noun</td>\n",
       "      <td>sreznevskiy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>абрѣдиѥ</td>\n",
       "      <td>актиды</td>\n",
       "      <td>noun</td>\n",
       "      <td>sreznevskiy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word definition        pos       sourse\n",
       "0    абидныи    обидный  adjective  sreznevskiy\n",
       "1     аблань     яблоня       noun  sreznevskiy\n",
       "2  абланьныи  яблоневый  adjective  sreznevskiy\n",
       "3     аблъко     яблоко       noun  sreznevskiy\n",
       "4    абрѣдиѥ     актиды       noun  sreznevskiy"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {\n",
    "    \"adjective\": \"ADJ\",\n",
    "    \"noun\": \"NOUN\",\n",
    "    \"adverb\": \"ADV\",\n",
    "    \"pronoun\": \"PRON\",\n",
    "    \"verb\": \"VERB\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_word(row):\n",
    "#     print(row)\n",
    "    line = row[\"definition\"]\n",
    "    if \" \" not in line:\n",
    "        return line\n",
    "    pos = pos_dict.get(row.get(\"pos\", None), None)\n",
    "#     print(pos)\n",
    "    stemmed = tag_mystem(text=line)\n",
    "#     print(stemmed)\n",
    "    imp_words = [word for word in stemmed if word in rusvec_model]\n",
    "#     print(imp_words)\n",
    "    if len(imp_words) == 0:\n",
    "        return line.split()[0]\n",
    "    single_vector = count_vector(imp_words)\n",
    "    if pos is None:\n",
    "        res = rusvec_model.similar_by_vector(single_vector, topn=1)\n",
    "#         print(res)\n",
    "        return res[0][0].split(\"_\")[0]\n",
    "    else:\n",
    "        guesses = rusvec_model.similar_by_vector(single_vector, topn=3)\n",
    "#         print(guesses)\n",
    "#         print(pos)\n",
    "        for guess, _ in guesses:\n",
    "#             print(guess)\n",
    "            if guess.endswith(pos):\n",
    "                return guess.split(\"_\")[0]\n",
    "        else:\n",
    "            return guesses[0][0].split(\"_\")[0]\n",
    "        \n",
    "# def count_vector(imp_words):\n",
    "#     vects = np.array([rusvec_model.wv[word] for word in imp_words])\n",
    "#     single_vector = vects.mean(axis=0)\n",
    "#     return single_vector\n",
    "\n",
    "def count_vector(imp_words):\n",
    "    vects = np.array([rusvec_model[word] for word in imp_words])\n",
    "    if len(imp_words) == 1:\n",
    "        return vects[0]\n",
    "    noun_mask = [word.endswith(\"_NOUN\") for word in imp_words]\n",
    "    single_vector = np.average(vects, axis=0, weights=count_coefs(noun_mask))\n",
    "    return single_vector\n",
    "\n",
    "def count_coefs(mask):\n",
    "    if not any(mask):\n",
    "        mask[1] = True\n",
    "    coefs = list()\n",
    "    usual_coef = 1 / len(mask)\n",
    "    for imp_mark in mask:\n",
    "        if imp_mark:\n",
    "            coef = usual_coef * 1.5\n",
    "        else:\n",
    "            coef = usual_coef\n",
    "        coefs.append(coef)\n",
    "    # softmax\n",
    "    coefs = np.exp(coefs)/sum(np.exp(coefs))\n",
    "    return coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'делать'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_word(or_dict.loc[20090])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "or_dict = or_dict.assign(translation=or_dict.apply(single_word, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"or_dict_translated.pickle\", \"wb\") as f:\n",
    "    pickle.dump(or_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = or_dict.loc[20050:20100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
